{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43137e73-1c36-434c-8671-501817fe10db",
   "metadata": {},
   "source": [
    "# <center> GBM VS AdaBoost </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac74591-29dd-4ae9-817e-3119d9c46829",
   "metadata": {},
   "source": [
    "1. **Como os erros são tratados**  \n",
    "Ada ajusta os pesos das amostras. Dá mais peso para os exemplos mal classificados, forçando o próximo modelo a focar neles. Enquanto no GBM, se modela os resíduos (erros) diretamente — o próximo modelo tenta prever o quanto o anterior errou.\n",
    "2. **Forma de atualização**  \n",
    "Ada usa ponderação dos exemplos e dos classificadores. Já o GBM usa gradientes da função de perda, aplicando conceitos de otimização para melhorar a performance.\n",
    "3. **Função de perda**  \n",
    "AdaBoost tem uma função de perda fixa (exponencial). Já o GBM permite várias funções de perda personalizáveis, como erro quadrático, log loss, etc.\n",
    "4. **Flexibilidade**  \n",
    "AdaBoost é mais simples e restrito. Normalmente usa árvores rasas (stumps). Enquanto que o GBM é muito mais flexível em termos de profundidade das árvores, funções de perda, e outras personalizações.\n",
    "5. **Desempenho e ajuste fino**  \n",
    "AdaBoost é menos propenso a overfitting, mas menos preciso em problemas complexos.Já o GBM pode alcançar alta precisão, mas requer mais cuidado com os hiperparâmetros para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42fedd3-3f0c-4e0f-b00b-d65c2d2fcb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d13a99-2465-4e46-b51a-ad228be52aa7",
   "metadata": {},
   "source": [
    "## 5 Hyperparametros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0973c0c-bd5a-472d-9076-127670c74958",
   "metadata": {},
   "source": [
    "1. n_estimators\t\n",
    "2. learning_rate\n",
    "3. max_depth\n",
    "4. subsample\n",
    "5. min_samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d8762-e90c-4853-a793-5bb4de04386c",
   "metadata": {},
   "source": [
    "## Stochastic GBM, qual é a maior diferença?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be50d3d-22f4-4de0-923f-744b4002865b",
   "metadata": {},
   "source": [
    "O Stochastic GBM usa amostragem aleatória dos dados de treino a cada iteração."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04037e18-6f72-451a-997b-6f6de7afd80b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
